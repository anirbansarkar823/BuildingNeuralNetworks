{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_using_LSTM_on_IMDB_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr4cugH3mZ_X"
      },
      "source": [
        "# Credits: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
        "# LSTM for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjf2yM5GF4_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80bc9ea-ee53-43b5-9dbb-e05b87f0e029"
      },
      "source": [
        "#Refer: https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification\n",
        "\n",
        "# load the dataset but will keep only the top 5000 words by frequency in each review, will keep zero for the rest words present in the review\n",
        "# so if a review has any word from top 5000 most frequent words, the corresponding index will be placed. \n",
        "# if the word of the review is not in top 5000 words, we will skip its index\n",
        "# a total of 25000 training data and 25000 test data\n",
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYEE6ts7GAjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352f0f69-9e75-4242-a787-8423100e1cf4"
      },
      "source": [
        "print(X_train[1])\n",
        "print(type(X_train[1]))\n",
        "print(len(X_train[1]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
            "<class 'list'>\n",
            "189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhWo1v1M7elZ",
        "outputId": "bd5ce26b-5607-4742-ddd4-90d82ffd7723"
      },
      "source": [
        "# Number of training samples\n",
        "X_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgBoibswH82e",
        "outputId": "c8138abe-24a9-4e51-9c2d-de9508053213"
      },
      "source": [
        "# Number of test samples\n",
        "X_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWLewj15KG_",
        "outputId": "29a6e2d1-9e5f-4754-8285-5a5e7692eb7a"
      },
      "source": [
        "import numpy as np\n",
        "np.unique(y_train) # positive and negative reviews"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn1coShH7LvP",
        "outputId": "35f7f686-6493-4b07-91c6-a6ee1c38f836"
      },
      "source": [
        "max(numpy.max(X_test))\n",
        "# as we have choosen top 5000 words no index is greater than 5000"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4998"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyVN1nViGWQ9",
        "outputId": "f82e9ef5-8d97-4758-deb0-c918794847e8"
      },
      "source": [
        "max(numpy.max(X_train))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4987"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojXLjQaUF0ek"
      },
      "source": [
        "#### Purpose of Padding\n",
        "* to convert each review, to 600 length vectors. \n",
        "  * case-1: if the number of words in the review is less than 600, pre-pad with 0's\n",
        "  * case-2: if the number of words are more than 600, keep only first 600 word's indexes\n",
        "\n",
        "* Padding helps to do SGD with batch size > 1. If we don't use Padding, we need to send one review at a time.\n",
        "  * As padding converts all the reviews to same size, we can use the same network to train using multiple reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCRyMDnjlOs0",
        "outputId": "d4a13908-f855-4ff3-fd9d-992d1a4456a2"
      },
      "source": [
        "## Before padding\n",
        "print(X_train.shape)\n",
        "print(X_train[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000,)\n",
            "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57N6TyKLH-Pc"
      },
      "source": [
        "# truncate and/or pad input sequences\n",
        "max_review_length = 600\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWbmPXuiHJzl",
        "outputId": "726264b0-c11a-4245-e799-12ac04d504dd"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(len(X_train[1]))\n",
        "print(X_train[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 600)\n",
            "600\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    1  194 1153  194    2   78  228    5    6\n",
            " 1463 4369    2  134   26    4  715    8  118 1634   14  394   20   13\n",
            "  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n",
            "   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n",
            "  116    9   35    2    4  229    9  340 1322    4  118    9    4  130\n",
            " 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n",
            "   43   38 1543 1905  398    4 1649   26    2    5  163   11 3215    2\n",
            "    4 1153    9  194  775    7    2    2  349 2637  148  605    2    2\n",
            "   15  123  125   68    2    2   15  349  165 4362   98    5    4  228\n",
            "    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n",
            "   50    9 4373  228    2    5    2  656  245 2350    5    4    2  131\n",
            "  152  491   18    2   32    2 1212   14    9    6  371   78   22  625\n",
            "   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n",
            "   28    6   52  154  462   33   89   78  285   16  145   95]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CquzlqrOIYGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4ebd15-5381-4374-afad-5c3ec3dd2a96"
      },
      "source": [
        "# creating the model\n",
        "embedding_vecor_length = 32\n",
        "top_words = 5000 # top frequencty\n",
        "max_review_length = 600 # padding length\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length)) \n",
        "# Embedding layer to converts  positive integers (indexes) into dense vectors of fixed size. [https://keras.io/api/layers/core_layers/embedding/], [https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/]\n",
        "# input_dim (first parameter, here top_words): Integer. Size of the vocabulary, in our case it is 600\n",
        "# output_dim (second parameter, here embedding_vector_length): Integer. Dimension of the dense embedding i.e., length of each vector\n",
        "# So each review {which has 600 indices} will be of size (600, 32) i.e., each word/word_index will become a 32 length vector. The resultant vector is a dense one with having real values instead of just 0's and 1's.\n",
        "\n",
        "model.add(LSTM(100)) # adding number of layers in deep LSTM\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid')) # final output layer to take input from each LSTM (100 are there)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model\n",
        "\n",
        "# (None, 600, 32) --> Each of the 600 indexes are converted to 32-length vectors. None is the batch size, which we supply during training\n",
        "# 160000 = 5000 * 32; for each 5000 n-frequent words, getting a vector of size 32\n",
        "# 53200 = 4 * (n*m + n*n + n); where n-number of LSTM layers, m-length of vector; +n - represents biases, 1 bias for each LSTM layer\n",
        "# number of parameters from LSTM = 4* (100 * 32 + 100 * 100 + 100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 600, 32)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               53200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_FT0dPNIeLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffb30fed-28a7-45ab-b71d-8ad2891a9979"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=64)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 273s 693ms/step - loss: 0.4728 - accuracy: 0.7698\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 271s 694ms/step - loss: 0.3003 - accuracy: 0.8804\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 271s 694ms/step - loss: 0.2444 - accuracy: 0.9042\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 271s 694ms/step - loss: 0.2661 - accuracy: 0.8936\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 271s 694ms/step - loss: 0.3946 - accuracy: 0.8128\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 271s 693ms/step - loss: 0.2151 - accuracy: 0.9164\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 269s 687ms/step - loss: 0.1878 - accuracy: 0.9296\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 271s 693ms/step - loss: 0.1576 - accuracy: 0.9424\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 271s 693ms/step - loss: 0.1340 - accuracy: 0.9520\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 271s 692ms/step - loss: 0.1650 - accuracy: 0.9366\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe23a6d5c10>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgVm16Yt-pef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29454860-6245-4ec4-d7ba-e2e216904928"
      },
      "source": [
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09dVSF8LLREd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}