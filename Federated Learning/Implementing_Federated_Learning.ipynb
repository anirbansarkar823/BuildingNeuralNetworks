{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Federated_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MrShdh54NF_7",
        "0k1yqEjoo5Jh",
        "K7DMdbRUROCH",
        "CvVvIjwV5Aet",
        "k5fhoAZTImJO",
        "inWQ-3OqkxG2",
        "Pt4LmYyAuO7q",
        "EizJ6Dty6ux7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC9emmDVrYiX"
      },
      "source": [
        "## About temporal and spatial coefficients:\n",
        "# - time is first converted to seconds, then time slot using slot-size (which was taken to be 300 s)\n",
        "# - the latitude, longitude values are converted to whole number value by multiplying required powers of 10\n",
        "\n",
        "# # Normalization - lat, long : (l - l_min); basically converting to a range [0, l_max - l_min]\n",
        "\n",
        "# # Formula for correlation:\n",
        "# dST (x, y) = α.dS (x, y) + (1 − α).dT\n",
        "\n",
        "# ** α can be set to a value a value greater than 0.5 ---> (spatial corr) > (temporal corr)\n",
        "\n",
        "# # KNN-ST algorithm:\n",
        "# Inputs: k, α and type. {α - [0,1]; type - [naive (deduced value is simply avg of k nearest neighbors), weighted (weighted avg of k n neighbors; weight is inversely prop to distance from the data point to be deduced), k - number of neighbors to consider]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Z9nqAPrfIM"
      },
      "source": [
        "# Algorithm\n",
        "# //k, α and type are input parameters.\n",
        "for each missing data-point x do\n",
        "  # //Find deduced value of x, xd\n",
        "  Get the values of the k nearest points into array S[1..k]\n",
        "  # values means?? distance or direct cordinate values?\n",
        "\n",
        "  and the associated d_ST distances into array D[1..k]; \n",
        "  # d_ST (x, y) = α.d_S (x, y) + (1 − α).d_T\n",
        "\n",
        "  x_d ← 0\n",
        "  if type = ′naive′ then\n",
        "    sum_s ← 0\n",
        "    \n",
        "    for i = 1 to k do\n",
        "      sum_s ← sum_s + S[i]\n",
        "    end for\n",
        "    xd ← sums/k # simple avg of kNN values\n",
        "\n",
        "  else #type is weighted avg\n",
        "    if type = ′weighted′ then\n",
        "      sum_s ← 0; sum_d ← 0\n",
        "      \n",
        "      for i = 1 to k do\n",
        "        sum_d ← sum_d + 1/D[i]\n",
        "      end for\n",
        "      \n",
        "      for i = 1 to k do\n",
        "        x_d ← x_d + ((1/D[i])/sum_D).S[i] # Does D and d have different significance?\n",
        "      end for\n",
        "\n",
        "    end if\n",
        "  end if\n",
        "end for"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JXo-MUf4o2H"
      },
      "source": [
        "#### Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t17--U9YztDl"
      },
      "source": [
        "      summation{i,j,t:S(i,j,t)=0 (|S(i, j, t) − Sˆ(i, j, t)|)} \n",
        "NE = ----------------------------------------------------------\n",
        "        summation{i,j,t:S(i,j,t)=0 (|S(i, j,t)|) }\n",
        "\n",
        "\n",
        "where, \n",
        "S(i, j, t) is the actual sensed value at voxel (i, j, t)\n",
        "Sˆ(i, j, t) is the deduced value at voxel (i, j, t). \n",
        "\n",
        "When, S(i, j, t) = 0, it means value at voxel (i, j, t) is missing (and has to be deduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2CMzEuT4rn1"
      },
      "source": [
        "#### Loss probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8W2h6jZ19tJ"
      },
      "source": [
        "loss probability of 0.8 represents 80% of data is missing."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvL2Jvg74mx1"
      },
      "source": [
        "ratioS = error(KNN-S)/(error(KNN-S)+error(KNN-T)). \n",
        "\n",
        "Here,\n",
        "error(KNN-S) and error(KNN-T) represent the error arrived\n",
        "at when KNN-S and KNN-T are applied respectively. \n",
        "\n",
        "Similarly, \n",
        "ratioT = error(KNN-T)/(error(KNN-S) + error(KNN-T)).\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nlPlEG1_r6O"
      },
      "source": [
        "#### The higher the error ratio, the lesser the correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E4M3ng9_zE6"
      },
      "source": [
        "## set α = 1−ratioS since α is the weight assigned to the spatial component. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oufRY9dJS4pG"
      },
      "source": [
        "### More on Federated Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goIkESrz0wSD",
        "outputId": "b1570d63-7a82-488f-d2d0-2864a14d0d93"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNhUiqXTS7YJ"
      },
      "source": [
        "# What I understood is, basically the server has connection with multiple clients (say mobile devices).\n",
        "# The problem was earlier, client had to send their data (say heath data, messages typed on keyboard) to server so that a model can be built using clients data\n",
        "# Here, privacy and protection of data was at stake\n",
        "\n",
        "# Now using FL, the server will initiate a model (based on know parameters to consider) using some weights and biases.\n",
        "# The same parameters will be sent to all the clients.\n",
        "# On client devices, a model will be built using the received parameters and then train and update it accordingly. This process will occur on each client\n",
        "# Once trained, the learned weight and biases will be sent server.\n",
        "# On server, all the weights and biases will be aggregated (say using mean/average) and then that aggreated updated weights and biases will again be sent to each clients\n",
        "# The above process will be repeated until we reach a optimal weights and biases [Although in real life, as new data gets generated everyday on client devices, the process should take place regularly to serve clients with the best model]\n",
        "# Throughtout the process, clients' data never leave their place.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkB6xhA6bztz"
      },
      "source": [
        "## How different from Distributed Learnings?\n",
        "# Definitely different from distributed learnings. In distributed learning, all the high-end devices are generally on same network inside a restricted area.\n",
        "# Distributed learing is thus much much faster than FL. (one because FL devices are on different networks; second devices have very less processing power)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGsthg4KbPRU"
      },
      "source": [
        "# Details of FedAvg\n",
        "  # FedAvg Loss: aim is to minimize overall global loss \n",
        "  # f(w) = summation(k=1 to K){(n_k / n) * F_k(w)}\n",
        "  # K - number of clients\n",
        "  # F_k - Client k's loss function; this is getting calculated on each client's individual devices\n",
        "  # n_k - weighted by size of client k's dataset; devices with larger dataset will have more losses\n",
        "  # Algorithm: FederatedAveraging. The K clients are\n",
        "            # indexed by k; B is the local mini-batch size, E is the number\n",
        "            # of local epochs, and η is the learning rate.\n",
        "\n",
        "  # Server executes:\n",
        "    initialize w_0\n",
        "    for each round t = 1, 2, . . . do\n",
        "      m ← max(C · K, 1)\n",
        "      S_t ← (random set of m clients)\n",
        "    \n",
        "      for each client k ∈ S_t in parallel do\n",
        "        w^(k)_(t+1) ← ClientUpdate(k, w_t)\n",
        "      w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "\n",
        "\n",
        "  # ClientUpdate(k, w): // Run on client k\n",
        "    B ← (split P_k into batches of size B) # what is p_K???\n",
        "    for each local epoch i from 1 to E do\n",
        "      for batch b ∈ B do\n",
        "        w ← w − η O l(w; b) # this seems the SGD process\n",
        "    \n",
        "    return w to server\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOG11qdKixyn"
      },
      "source": [
        "# Improvements on FedAvg\n",
        "  # Problem with FedAvg: 1. assumption that all local devices will have 'E' epochs of SGD; as a result some devices takes longer time than others\n",
        "                      # 2. FedAvg ends up prefering devices with more data\n",
        "              \n",
        "  # 1st improvement: FedProx:\n",
        "                          # 1. adds a regularization term to penalize large change in weights\n",
        "\n",
        "  # and many more improvements are there\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OthrQIBykB1Y"
      },
      "source": [
        "# # Frameworks and datasets\n",
        "#   1. TensorFlow Federated: Machine Learning on Decentralized Data\n",
        "#   2. PyTorch: PySyft\n",
        "#   3. Flower: pip install flwr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrShdh54NF_7"
      },
      "source": [
        "# 20th Sept, 2021\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-47fc5iCKyjA"
      },
      "source": [
        "[Gradient Descent in LR](https://miro.medium.com/max/450/1*G3evFxIAlDchOx5Wl7bV5g.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03TGv8PGJaeN"
      },
      "source": [
        "### Batched Gradient Descent\n",
        "*  In the batch gradient descent, to calculate the gradient of the cost function, we need to sum all training examples for each steps\n",
        "* If we have 3 millions samples (m training examples) then the gradient descent algorithm should sum 3 millions samples for every epoch. To move a single step, we have to calculate each with 3 million times!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWVu_jrTJZuu"
      },
      "source": [
        "def gradientDescent(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "       Performs gradient descent to learn theta\n",
        "       theta: t0 (y-intercept), t1 (slope)\n",
        "    \"\"\"\n",
        "    m = y.size  # number of training examples\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        y_hat = np.dot(X, theta)\n",
        "        theta = theta - alpha * (1.0/m) * np.dot(X.T, y_hat-y)\n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ub4Em8qL42c"
      },
      "source": [
        "### Stochastic Gradient Descent (SGD)\n",
        "* In stochastic Gradient Descent, we use one example or one training sample at each iteration instead of using whole dataset to sum all for every steps\n",
        "* SGD is widely used for larger dataset trainings and computationally faster and can be trained in parallel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AokRRH-rL_M_"
      },
      "source": [
        "def SGD(f, theta0, alpha, num_iters):\n",
        "    \"\"\" \n",
        "       Arguments:\n",
        "       f -- the function to optimize, it takes a single argument\n",
        "            and yield two outputs, a cost and the gradient\n",
        "            with respect to the arguments\n",
        "       theta0 -- the initial point to start SGD from\n",
        "       num_iters -- total iterations to run SGD for\n",
        "       Return:\n",
        "       theta -- the parameter value after SGD finishes\n",
        "    \"\"\"\n",
        "    start_iter = 0\n",
        "    theta= theta0\n",
        "    for iter in xrange(start_iter + 1, num_iters + 1):\n",
        "        _, grad = f(theta)\n",
        "        theta = theta - (alpha * grad) # there is NO dot product!\n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbn_SmGJMdbj"
      },
      "source": [
        "### Mini-Batch Gradient Descent\n",
        "* It is similar like SGD, it uses n samples instead of 1 at each iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k1yqEjoo5Jh"
      },
      "source": [
        "# 22nd September, 2021\n",
        "#### Implementing the Algorithm using python code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QueYu3Mpo8HJ"
      },
      "source": [
        "# Details of FedAvg\n",
        "  # FedAvg Loss: aim is to minimize overall global loss \n",
        "  # f(w) = summation(k=1 to K){(n_k / n) * F_k(w)}\n",
        "  # K - number of clients\n",
        "  # F_k - Client k's loss function; this is getting calculated on each client's individual devices\n",
        "  # n_k - weighted by size of client k's dataset; devices with larger dataset will have more losses\n",
        "  # Algorithm: FederatedAveraging. The K clients are\n",
        "            # indexed by k; B is the local mini-batch size, E is the number\n",
        "            # of local epochs, and η is the learning rate.\n",
        "\n",
        "K = 25 # as we have 25 clients\n",
        "clients = {} # dictionary\n",
        "\n",
        "# reading all the clients\n",
        "for st_code in range(101,126):\n",
        "    clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Server executes: \n",
        "def Server():\n",
        "  # initialize w_0\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=2))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, kernel_initializer=init_mode, activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='mse',\n",
        "            optimizer='SGD',\n",
        "            metrics=['mse'])\n",
        "  \n",
        "  # return model\n",
        "\n",
        "  # for each round t = 1, 2, . . . do\n",
        "  for t in range(1, 5):\n",
        "    C = np.random.random(1)[0]\n",
        "\n",
        "    # m ← max(C  K, 1)\n",
        "    m = max(int(C*K), 1) # m == random number of client selected\n",
        "    weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
        "    n_k = [None] * m # parameters for weighted sum \n",
        "\n",
        "\n",
        "    # S_t ← (random set of m clients)\n",
        "    S = {} # dictionary\n",
        "    m_clients = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "    for client in m_clients:\n",
        "      S[client] = clients[client]\n",
        "    \n",
        "    initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "    # for each client k ∈ S_t in parallel do\n",
        "    client_idx = 0\n",
        "    for client in S:\n",
        "      # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
        "      n_k[client_idx] = Client(client, model) # pass by reference\n",
        "      weight_t_plus_1[client_idx] = model.get_weights()\n",
        "      client_idx += 1\n",
        "      \n",
        "      # setting weights back to initial weights\n",
        "      model.set_weights(initial_weights)\n",
        "\n",
        "    final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0]/ sum(n_k))\n",
        "\n",
        "    for idx in range(1, len(weight_t_plus_1)):\n",
        "      # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "      final_weights_t += np.array(weight_t_plus_1[0]) * (n_k[idx]/ sum(n_k))\n",
        "\n",
        "    # setting the aggregated weights\n",
        "    model.set_weights(final_weights_t)\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ClientUpdate(k, w): // Run on client k\n",
        "def Client(client, model):\n",
        "  B ← (split P_k into batches of size B) # what is p_K???\n",
        "  for each local epoch i from 1 to E do\n",
        "    for batch b ∈ B do\n",
        "      w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "   return w to server"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7DMdbRUROCH"
      },
      "source": [
        "# 24th Sept, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-DjpqINRRyk"
      },
      "source": [
        "# Details of FedAvg\n",
        "  # FedAvg Loss: aim is to minimize overall global loss \n",
        "  # f(w) = summation(k=1 to K){(n_k / n) * F_k(w)}\n",
        "  # K - number of clients\n",
        "  # F_k - Client k's loss function; this is getting calculated on each client's individual devices\n",
        "  # n_k - weighted by size of client k's dataset; devices with larger dataset will have more losses\n",
        "  # Algorithm: FederatedAveraging. The K clients are\n",
        "            # indexed by k; B is the local mini-batch size, E is the number\n",
        "            # of local epochs, and η is the learning rate.\n",
        "\n",
        "K = 25 # as we have 25 clients\n",
        "clients = {} # dictionary\n",
        "\n",
        "# reading all the clients\n",
        "for st_code in range(101,126):\n",
        "    clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\")\n",
        "\n",
        "\n",
        "\n",
        "# Server executes: \n",
        "def Server():\n",
        "  # initialize w_0\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=2))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, kernel_initializer=init_mode, activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "  # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "  # for each round t = 1, 2, . . . do\n",
        "  for t in range(1, 5):\n",
        "    C = np.random.random(1)[0] # random number between 0 and 1.\n",
        "\n",
        "    # m ← max(C  K, 1)\n",
        "    m = max(int(C*K), 1) # m == random number of client selected\n",
        "    weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
        "    n_k = [None] * m # parameters for weighted sum \n",
        "\n",
        "\n",
        "    # S_t ← (random set of m clients)\n",
        "    # S = {} # dictionary\n",
        "    S_t = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "\n",
        "    # No need for below loop, as server don't need access to client data, server only need clients' number\n",
        "    # for client in m_clients:\n",
        "    #   S[client] = clients[client]\n",
        "    \n",
        "    initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "    # for t= 1 to T, initial_weights would be t-1th's final weights\n",
        "\n",
        "    # for each client k ∈ S_t in parallel do\n",
        "    client_idx = 0\n",
        "    for client in S_t:\n",
        "      # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
        "      n_k[client_idx] = Client(client, model) # pass by reference\n",
        "      weight_t_plus_1[client_idx] = model.get_weights()\n",
        "      client_idx += 1\n",
        "      \n",
        "      # setting weights back to initial weights\n",
        "      model.set_weights(initial_weights)\n",
        "\n",
        "    # finding the weighted sum\n",
        "    final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0] / sum(n_k))\n",
        "\n",
        "    for idx in range(1, m):\n",
        "      # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "      final_weights_t += np.array(weight_t_plus_1[idx]) * (n_k[idx]/ sum(n_k))\n",
        "\n",
        "    # setting the aggregated weights\n",
        "    model.set_weights(final_weights_t)\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# ClientUpdate(k, w): // Run on client k\n",
        "def Client(client_idx, model):\n",
        "# B ← (split P_k into batches of size B) # what is p_K --> each clients' sample size\n",
        "  B = len(clients[client_idx]) / 1000 # so for each client the batch size would be different depending upon the total sample size\n",
        "  E = 30 + B # number of local epochs; it will also depend on sample size (indirectly)\n",
        "  learning_rate = 0.1 # initial learning rate\n",
        "  decay_rate = 0.1\n",
        "  momentum = 0.8\n",
        "\n",
        "  # define the optimizer function\n",
        "  sgd = SGD(lr = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(loss='mse', \n",
        "                optimizer=sgd, \n",
        "                metrics=['mse'])\n",
        "  \n",
        "\n",
        "# for each local epoch i from 1 to E do\n",
        "#   for batch b ∈ B do\n",
        "#     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "  # fitting the model: same as above two for loops\n",
        "  dataset = clients[client_idx]\n",
        "  X, y = \n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=1)\n",
        "  keras_model.fit(x_train, y_train, \n",
        "                  epochs = E, \n",
        "                  batch_size=B,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  verbose=True)\n",
        "\n",
        "  # return w to server\n",
        "  # in our case no need for any explicit return; as the model is pass by value\n",
        "  return dataset.shape[0] # this will return the number of total sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVvIjwV5Aet"
      },
      "source": [
        "# 29th Sept, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCWojUBdHU6w",
        "outputId": "d2a2efdb-8dfa-4917-cc4b-7a683967224a"
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9YV_H0zF4S_",
        "outputId": "4204d754-aa6e-4425-ccdc-bf4151bf9e55"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.40.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "212em6QbH-ev",
        "outputId": "799475b1-25e2-431b-9f19-3bd3851c9456"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n",
            "2.3.0-tf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yh8HC4cIMKM"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwbzH9KNx4_Q"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import utils\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Activation\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD # from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import RMSprop # instead of from keras.optimizers import RMSprop\n",
        "from keras import datasets\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import History\n",
        "\n",
        "from keras import losses\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8qYD1UjIo75",
        "outputId": "a9517ddd-abb0-41d3-a662-61c09c07a61f"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "# import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD # from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import RMSprop # instead of from keras.optimizers import RMSprop\n",
        "# from keras import datasets\n",
        "\n",
        "# from keras.callbacks import LearningRateScheduler\n",
        "# from keras.callbacks import History\n",
        "\n",
        "from tensorflow.keras import losses\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.0\n",
            "2.3.0-tf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p00oZ-Qk5HOE"
      },
      "source": [
        "# Details of FedAvg\n",
        "  # FedAvg Loss: aim is to minimize overall global loss \n",
        "  # f(w) = summation(k=1 to K){(n_k / n) * F_k(w)}\n",
        "  # K - number of clients\n",
        "  # F_k - Client k's loss function; this is getting calculated on each client's individual devices\n",
        "  # n_k - weighted by size of client k's dataset; devices with larger dataset will have more losses\n",
        "  # Algorithm: FederatedAveraging. The K clients are\n",
        "            # indexed by k; B is the local mini-batch size, E is the number\n",
        "            # of local epochs, and η is the learning rate.\n",
        "\n",
        "K = 25 # as we have 25 clients\n",
        "clients = {} # dictionary\n",
        "\n",
        "# reading all the clients\n",
        "for st_code in range(101,126):\n",
        "    clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\", index_col = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDanqfTg6RMd",
        "outputId": "9aa4273b-fe5a-480c-a281-2e61c2a992e2"
      },
      "source": [
        "4//2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU3e68dW_CFB",
        "outputId": "4cdbeddc-5b85-49fc-beee-bcc79ada4e99"
      },
      "source": [
        "# import pandas as pd\n",
        "# df = pd.DataFrame({'datetime':pd.date_range('2020-01-01 07:00',periods=6)})\n",
        "# print(\"DataFrame is:\\n\", df)\n",
        "# for d in df['datetime']:\n",
        "#    df['date'] = d.date()\n",
        "#    df['time'] = d.time()\n",
        "# print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame is:\n",
            "              datetime\n",
            "0 2020-01-01 07:00:00\n",
            "1 2020-01-02 07:00:00\n",
            "2 2020-01-03 07:00:00\n",
            "3 2020-01-04 07:00:00\n",
            "4 2020-01-05 07:00:00\n",
            "5 2020-01-06 07:00:00\n",
            "             datetime        date      time\n",
            "0 2020-01-01 07:00:00  2020-01-06  07:00:00\n",
            "1 2020-01-02 07:00:00  2020-01-06  07:00:00\n",
            "2 2020-01-03 07:00:00  2020-01-06  07:00:00\n",
            "3 2020-01-04 07:00:00  2020-01-06  07:00:00\n",
            "4 2020-01-05 07:00:00  2020-01-06  07:00:00\n",
            "5 2020-01-06 07:00:00  2020-01-06  07:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA-3FZ4O-qVr"
      },
      "source": [
        "# Updating time column\n",
        "for station_id in range(101,126):\n",
        "  clients[station_id]['Measurement date'] = pd.to_datetime(clients[station_id]['Measurement date']) #changing the data type of column\n",
        "  for dt in clients[station_id]['Measurement date']:\n",
        "    clients[station_id]['date'], clients[station_id]['time'] = dt.date(), dt.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "wMVOeiOa5hAq",
        "outputId": "69ba1246-df24-4abb-da28-6b0f0ecccbc4"
      },
      "source": [
        "clients[101].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Measurement date</th>\n",
              "      <th>Station code</th>\n",
              "      <th>Address</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>SO2</th>\n",
              "      <th>NO2</th>\n",
              "      <th>O3</th>\n",
              "      <th>CO</th>\n",
              "      <th>PM10</th>\n",
              "      <th>PM2.5</th>\n",
              "      <th>date</th>\n",
              "      <th>time</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01 00:00:00</td>\n",
              "      <td>101</td>\n",
              "      <td>19, Jong-ro 35ga-gil, Jongno-gu, Seoul, Republ...</td>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.2</td>\n",
              "      <td>73.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-01 01:00:00</td>\n",
              "      <td>101</td>\n",
              "      <td>19, Jong-ro 35ga-gil, Jongno-gu, Seoul, Republ...</td>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.2</td>\n",
              "      <td>71.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-01 02:00:00</td>\n",
              "      <td>101</td>\n",
              "      <td>19, Jong-ro 35ga-gil, Jongno-gu, Seoul, Republ...</td>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.2</td>\n",
              "      <td>70.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-01 03:00:00</td>\n",
              "      <td>101</td>\n",
              "      <td>19, Jong-ro 35ga-gil, Jongno-gu, Seoul, Republ...</td>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.2</td>\n",
              "      <td>70.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-01 04:00:00</td>\n",
              "      <td>101</td>\n",
              "      <td>19, Jong-ro 35ga-gil, Jongno-gu, Seoul, Republ...</td>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.2</td>\n",
              "      <td>69.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Measurement date  Station code  ... month  year\n",
              "0 2017-01-01 00:00:00           101  ...     1  2017\n",
              "1 2017-01-01 01:00:00           101  ...     1  2017\n",
              "2 2017-01-01 02:00:00           101  ...     1  2017\n",
              "3 2017-01-01 03:00:00           101  ...     1  2017\n",
              "4 2017-01-01 04:00:00           101  ...     1  2017\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKDm8GjQ6U1U",
        "outputId": "f3ed32d3-3520-4509-e4ec-317d353b78ca"
      },
      "source": [
        "clients[101].columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Measurement date', 'Station code', 'Address', 'Latitude', 'Longitude',\n",
              "       'SO2', 'NO2', 'O3', 'CO', 'PM10', 'PM2.5', 'date', 'time', 'day',\n",
              "       'month', 'year'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjpBpzc47tpH",
        "outputId": "9451b49e-1985-4207-e061-2899c980ee43"
      },
      "source": [
        "clients[101].info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 25905 entries, 0 to 25904\n",
            "Data columns (total 16 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   Measurement date  25905 non-null  datetime64[ns]\n",
            " 1   Station code      25905 non-null  int64         \n",
            " 2   Address           25905 non-null  object        \n",
            " 3   Latitude          25905 non-null  float64       \n",
            " 4   Longitude         25905 non-null  float64       \n",
            " 5   SO2               25905 non-null  float64       \n",
            " 6   NO2               25905 non-null  float64       \n",
            " 7   O3                25905 non-null  float64       \n",
            " 8   CO                25905 non-null  float64       \n",
            " 9   PM10              25905 non-null  float64       \n",
            " 10  PM2.5             25905 non-null  float64       \n",
            " 11  date              25905 non-null  object        \n",
            " 12  time              25905 non-null  object        \n",
            " 13  day               25905 non-null  int64         \n",
            " 14  month             25905 non-null  int64         \n",
            " 15  year              25905 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(8), int64(4), object(3)\n",
            "memory usage: 3.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jcFFQn157jC"
      },
      "source": [
        "# taking required featuring and dropping rest\n",
        "for station_id in range(101,126):\n",
        "  clients[station_id] = clients[station_id][['Latitude', 'Longitude','day', 'month', 'year','CO']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiyUXIeW9W3D",
        "outputId": "e0aebc73-e3e2-481d-ade9-b18554c23ebe"
      },
      "source": [
        "t = pd.to_datetime(\"2017-04-21 05:00\")\n",
        "type(t.month)\n",
        "# t.day\n",
        "# type(t.year)\n",
        "# t.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "weiHreju7P5S",
        "outputId": "40564bcd-9c2b-451d-c238-2685033e0d18"
      },
      "source": [
        "clients[101]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>time</th>\n",
              "      <th>date</th>\n",
              "      <th>CO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25900</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25901</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25902</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25903</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25904</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>23:00:00</td>\n",
              "      <td>2019-12-31</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25905 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Latitude   Longitude      time        date   CO\n",
              "0      37.572016  127.005007  23:00:00  2019-12-31  1.2\n",
              "1      37.572016  127.005007  23:00:00  2019-12-31  1.2\n",
              "2      37.572016  127.005007  23:00:00  2019-12-31  1.2\n",
              "3      37.572016  127.005007  23:00:00  2019-12-31  1.2\n",
              "4      37.572016  127.005007  23:00:00  2019-12-31  1.2\n",
              "...          ...         ...       ...         ...  ...\n",
              "25900  37.572016  127.005007  23:00:00  2019-12-31  0.5\n",
              "25901  37.572016  127.005007  23:00:00  2019-12-31  0.4\n",
              "25902  37.572016  127.005007  23:00:00  2019-12-31  0.4\n",
              "25903  37.572016  127.005007  23:00:00  2019-12-31  0.5\n",
              "25904  37.572016  127.005007  23:00:00  2019-12-31  0.5\n",
              "\n",
              "[25905 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qupnipw5C8m"
      },
      "source": [
        "# ClientUpdate(k, w): // Run on client k\n",
        "def Client(client_idx, model):\n",
        "# B ← (split P_k into batches of size B) # what is p_K --> each clients' sample size\n",
        "  B = len(clients[client_idx]) / 1000 # so for each client the batch size would be different depending upon the total sample size\n",
        "  E = 30 + B # number of local epochs; it will also depend on sample size (indirectly)\n",
        "  learning_rate = 0.1 # initial learning rate\n",
        "  decay_rate = 0.1\n",
        "  momentum = 0.8\n",
        "\n",
        "  # define the optimizer function\n",
        "  sgd = SGD(lr = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(loss='mse', \n",
        "                optimizer=sgd, \n",
        "                metrics=['mse'])\n",
        "  \n",
        "\n",
        "# for each local epoch i from 1 to E do\n",
        "#   for batch b ∈ B do\n",
        "#     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "  # fitting the model: same as above two for loops\n",
        "  dataset = clients[client_idx]\n",
        "  X, y = dataset[['Latitude', 'Longitude','day', 'month', 'year']].copy(),  dataset['CO']\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=1)\n",
        "\n",
        "  model.fit(x_train, y_train, \n",
        "                  epochs = E, \n",
        "                  batch_size=B,\n",
        "                  validation_data=(x_test, y_test),\n",
        "                  verbose=True)\n",
        "\n",
        "  # return w to server\n",
        "  # in our case no need for any explicit return; as the model is pass by value\n",
        "  return dataset.shape[0] # this will return the number of total sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzusaow2CDN4"
      },
      "source": [
        "# Server executes: \n",
        "def Server():\n",
        "  # initialize w_0\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=2))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, kernel_initializer= 'he_uniform', activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "  # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "  # for each round t = 1, 2, . . . do\n",
        "  for t in range(1, 5):\n",
        "    C = np.random.random(1)[0] # random number between 0 and 1.\n",
        "\n",
        "    # m ← max(C  K, 1)\n",
        "    m = max(int(C*K), 1) # m == random number of client selected\n",
        "    weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
        "    n_k = [None] * m # parameters for weighted sum \n",
        "\n",
        "\n",
        "    # S_t ← (random set of m clients)\n",
        "    # S = {} # dictionary\n",
        "    S_t = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "\n",
        "    # No need for below loop, as server don't need access to client data, server only need clients' number\n",
        "    # for client in m_clients:\n",
        "    #   S[client] = clients[client]\n",
        "    \n",
        "    initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "    # for t= 1 to T, initial_weights would be t-1th's final weights\n",
        "\n",
        "    # for each client k ∈ S_t in parallel do\n",
        "    client_idx = 0\n",
        "    for client in S_t:\n",
        "      # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
        "      n_k[client_idx] = Client(client, model) # pass by reference\n",
        "      weight_t_plus_1[client_idx] = model.get_weights()\n",
        "      client_idx += 1\n",
        "      \n",
        "      # setting weights back to initial weights\n",
        "      model.set_weights(initial_weights)\n",
        "\n",
        "    # finding the weighted sum\n",
        "    final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0] / sum(n_k))\n",
        "\n",
        "    for idx in range(1, m):\n",
        "      # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "      final_weights_t += np.array(weight_t_plus_1[idx]) * (n_k[idx]/ sum(n_k))\n",
        "\n",
        "    # setting the aggregated weights\n",
        "    model.set_weights(final_weights_t)\n",
        "\n",
        "  print(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "a5VgIsXuD3ZJ",
        "outputId": "e175b940-7f9b-4605-dbd7-3148da055226"
      },
      "source": [
        "Server()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-ea80530cd932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-fe6b2f8c59e8>\u001b[0m in \u001b[0;36mServer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mS_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;31m# w^(k)_(t+1) ← ClientUpdate(k, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mn_k\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pass by reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m       \u001b[0mweight_t_plus_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mclient_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-7d15f1f5bb57>\u001b[0m in \u001b[0;36mClient\u001b[0;34m(client_idx, model)\u001b[0m\n\u001b[1;32m     31\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   verbose=True)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# return w to server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m       \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, count)\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \"\"\"\n\u001b[0;32m-> 1262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mRepeatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, count)\u001b[0m\n\u001b[1;32m   4408\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4409\u001b[0m       self._count = ops.convert_to_tensor(\n\u001b[0;32m-> 4410\u001b[0;31m           count, dtype=dtypes.int64, name=\"count\")\n\u001b[0m\u001b[1;32m   4411\u001b[0m     variant_tensor = gen_dataset_ops.repeat_dataset(\n\u001b[1;32m   4412\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert 55.903999999999996 to EagerTensor of dtype int64"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5fhoAZTImJO"
      },
      "source": [
        "# 30th sept, 2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHHvWNJmI3dc"
      },
      "source": [
        "# Details of FedAvg\n",
        "  # FedAvg Loss: aim is to minimize overall global loss \n",
        "  # f(w) = summation(k=1 to K){(n_k / n) * F_k(w)}\n",
        "  # K - number of clients\n",
        "  # F_k - Client k's loss function; this is getting calculated on each client's individual devices\n",
        "  # n_k - weighted by size of client k's dataset; devices with larger dataset will have more losses\n",
        "  # Algorithm: FederatedAveraging. The K clients are\n",
        "            # indexed by k; B is the local mini-batch size, E is the number\n",
        "            # of local epochs, and η is the learning rate.\n",
        "\n",
        "K = 25 # as we have 25 clients\n",
        "clients = {} # dictionary\n",
        "\n",
        "# reading all the clients\n",
        "for st_code in range(101,126):\n",
        "    clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\", index_col = 0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5csCtyMFJEzK"
      },
      "source": [
        "# taking required featuring and dropping rest\n",
        "for station_id in range(101,126):\n",
        "  clients[station_id] = clients[station_id][['Latitude', 'Longitude','day', 'month', 'year','CO']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD5vv7FZJVYo"
      },
      "source": [
        "# ClientUpdate(k, w): // Run on client k\n",
        "def Client(client_idx, model):\n",
        "# B ← (split P_k into batches of size B) # what is p_K --> each clients' sample size\n",
        "  B = 1 #int(len(clients[client_idx]) * 0.1) # so for each client the batch size would be different depending upon the total sample size\n",
        "  E = 1 #max(30, B/1 # number of local epochs; it will also depend on sample size (indirectly)\n",
        "  learning_rate = 0.1 # initial learning rate\n",
        "  decay_rate = 0.1\n",
        "  momentum = 0.8\n",
        "\n",
        "  # define the optimizer function\n",
        "  sgd = SGD(lr = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(loss='mse', \n",
        "                optimizer=sgd, \n",
        "                metrics=['mse'])\n",
        "  \n",
        "\n",
        "# for each local epoch i from 1 to E do\n",
        "#   for batch b ∈ B do\n",
        "#     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "  # fitting the model: same as above two for loops\n",
        "  dataset = clients[client_idx]\n",
        "  \n",
        "  X, y = dataset[['Latitude', 'Longitude','day', 'month', 'year']].copy(),  dataset['CO']\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=1)\n",
        "\n",
        "  try:\n",
        "    model.fit(x_train, y_train, \n",
        "                    epochs = E, \n",
        "                    batch_size=B,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    verbose=True)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    print(\"tata bye bye\")\n",
        "\n",
        "  # return w to server\n",
        "  # in our case no need for any explicit return; as the model is pass by value\n",
        "  return dataset.shape[0] # this will return the number of total sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz0H0x_qJeC0"
      },
      "source": [
        "# Server executes: \n",
        "def Server():\n",
        "  # initialize w_0\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=5))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, kernel_initializer= 'he_uniform', activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "  # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "  # for each round t = 1, 2, . . . do\n",
        "  for t in range(1, 5):\n",
        "    C = np.random.random(1)[0] # random number between 0 and 1.\n",
        "\n",
        "    # m ← max(C  K, 1)\n",
        "    m = max(int(C*K), 1) # m == random number of client selected\n",
        "    weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
        "    n_k = [None] * m # parameters for weighted sum \n",
        "\n",
        "\n",
        "    # S_t ← (random set of m clients)\n",
        "    # S = {} # dictionary\n",
        "    S_t = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "\n",
        "    # No need for below loop, as server don't need access to client data, server only need clients' number\n",
        "    # for client in m_clients:\n",
        "    #   S[client] = clients[client]\n",
        "    \n",
        "    initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "    # for t= 1 to T, initial_weights would be t-1th's final weights\n",
        "\n",
        "    # for each client k ∈ S_t in parallel do\n",
        "    client_idx = 0\n",
        "    for client in S_t:\n",
        "      # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
        "      n_k[client_idx] = Client(client, model) # pass by reference\n",
        "      weight_t_plus_1[client_idx] = model.get_weights()\n",
        "      client_idx += 1\n",
        "      \n",
        "      # setting weights back to initial weights\n",
        "      model.set_weights(initial_weights)\n",
        "\n",
        "    # finding the weighted sum\n",
        "    final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0] / sum(n_k))\n",
        "\n",
        "    for idx in range(1, m):\n",
        "      # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "      final_weights_t += np.array(weight_t_plus_1[idx]) * (n_k[idx]/ sum(n_k))\n",
        "\n",
        "    # setting the aggregated weights\n",
        "    model.set_weights(final_weights_t)\n",
        "\n",
        "  print(model.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWLrXbwwJfj3"
      },
      "source": [
        "Server()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioj4j6660PRN"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lzUPM-Hycjv"
      },
      "source": [
        "loss_probability = (missing values)/(total rows)\n",
        "# 0.1 --> 10/100\n",
        "NE = summation(abs(y_true - y_predicted)) / summation(y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inWQ-3OqkxG2"
      },
      "source": [
        "# 1st october : Both FL and Traditional ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbIEiKA9mDUO",
        "outputId": "fca63178-8df8-4db3-c3a1-6533119fc01a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6leGAYFk4Dc",
        "outputId": "e8e46bd3-8965-43ae-fd66-d2e8fd8a7728"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "# import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD # from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import RMSprop # instead of from keras.optimizers import RMSprop\n",
        "# from keras import datasets\n",
        "\n",
        "# from keras.callbacks import LearningRateScheduler\n",
        "# from keras.callbacks import History\n",
        "\n",
        "from tensorflow.keras import losses\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n",
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlsNS5L1licl",
        "outputId": "36bb7474-799d-4749-8e64-a01ddd05c078"
      },
      "source": [
        "%%time\n",
        "# getting input data\n",
        "K = 25 # as we have 25 clients\n",
        "clients = {} # dictionary\n",
        "\n",
        "# reading all the clients\n",
        "for st_code in range(101,126):\n",
        "    clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\", index_col = 0)\n",
        "\n",
        "    clients[st_code]['Measurement date'] = pd.to_datetime(clients[st_code]['Measurement date'])\n",
        "\n",
        "    clients[st_code]['year'] = clients[st_code]['Measurement date'].dt.year\n",
        "    clients[st_code]['month'] = clients[st_code]['Measurement date'].dt.month\n",
        "    clients[st_code]['week'] = clients[st_code]['Measurement date'].dt.week\n",
        "    clients[st_code]['day'] = clients[st_code]['Measurement date'].dt.day\n",
        "    clients[st_code]['hour'] = clients[st_code]['Measurement date'].dt.hour\n",
        "    clients[st_code]['minute'] = clients[st_code]['Measurement date'].dt.minute # minute is not significant; as only 0 values\n",
        "    clients[st_code]['dayOfWeek'] = clients[st_code]['Measurement date'].dt.dayofweek\n",
        "\n",
        "    # choosing features\n",
        "    clients[st_code] = clients[st_code][['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek', 'CO']]\n",
        "\n",
        "\n",
        "\n",
        "# dataset['date'], dataset['time'] = dataset['Measurement date'].dt.date, dataset['Measurement date'].dt.time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.59 s, sys: 233 ms, total: 2.82 s\n",
            "Wall time: 20.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZN2oEn1qnOF7",
        "outputId": "b2ce59f9-c7ef-4faa-f052-0b845ffed835"
      },
      "source": [
        "# merging all the clients data\n",
        "import pandas as pd\n",
        "\n",
        "frames = list(clients.values())\n",
        "\n",
        "dataset = pd.concat(frames)\n",
        "display(dataset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>week</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>dayOfWeek</th>\n",
              "      <th>CO</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37.572016</td>\n",
              "      <td>127.005007</td>\n",
              "      <td>2017</td>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647506</th>\n",
              "      <td>37.544962</td>\n",
              "      <td>127.136792</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647507</th>\n",
              "      <td>37.544962</td>\n",
              "      <td>127.136792</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647508</th>\n",
              "      <td>37.544962</td>\n",
              "      <td>127.136792</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647509</th>\n",
              "      <td>37.544962</td>\n",
              "      <td>127.136792</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>22</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647510</th>\n",
              "      <td>37.544962</td>\n",
              "      <td>127.136792</td>\n",
              "      <td>2019</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>31</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>647511 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Latitude   Longitude  year  month  week  day  hour  dayOfWeek   CO\n",
              "0       37.572016  127.005007  2017      1    52    1     0          6  1.2\n",
              "1       37.572016  127.005007  2017      1    52    1     1          6  1.2\n",
              "2       37.572016  127.005007  2017      1    52    1     2          6  1.2\n",
              "3       37.572016  127.005007  2017      1    52    1     3          6  1.2\n",
              "4       37.572016  127.005007  2017      1    52    1     4          6  1.2\n",
              "...           ...         ...   ...    ...   ...  ...   ...        ...  ...\n",
              "647506  37.544962  127.136792  2019     12     1   31    19          1  0.5\n",
              "647507  37.544962  127.136792  2019     12     1   31    20          1  0.4\n",
              "647508  37.544962  127.136792  2019     12     1   31    21          1  0.4\n",
              "647509  37.544962  127.136792  2019     12     1   31    22          1  0.5\n",
              "647510  37.544962  127.136792  2019     12     1   31    23          1  0.5\n",
              "\n",
              "[647511 rows x 9 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy-VNMXl2Vw7",
        "outputId": "ba00a7fa-4eef-44f3-d61d-ee8ebdf8eef7"
      },
      "source": [
        "X, y = dataset.drop('CO', axis=1),  dataset['CO']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=0)\n",
        "x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "x_test[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIoTgWrUmi7X"
      },
      "source": [
        "# ClientUpdate(k, w): // Run on client k\n",
        "def Client(client_idx, model, loss_p):\n",
        "    # B ← (split P_k into batches of size B) # what is p_K --> each clients' sample size\n",
        "    B = 1000 #int(len(clients[client_idx]) * 0.01) # so for each client the batch size would be different depending upon the total sample size\n",
        "    E = 12 #max(30, B//10) # number of local epochs; it will also depend on sample size (indirectly)\n",
        "    learning_rate = 0.1 # initial learning rate\n",
        "    decay_rate = 0.1\n",
        "    momentum = 0.8\n",
        "\n",
        "    # define the optimizer function\n",
        "    sgd = SGD(learning_rate = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(loss='mse', \n",
        "                optimizer=sgd, \n",
        "                metrics=['mse'])\n",
        "\n",
        "\n",
        "    # for each local epoch i from 1 to E do\n",
        "    #   for batch b ∈ B do\n",
        "    #     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "    # fitting the model: same as above two for loops\n",
        "    data = clients[client_idx]\n",
        "    # data = dataset.loc[dataset['Station code'] == client_idx]\n",
        "\n",
        "    X, y = data[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  data['CO']\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = loss_p, random_state=1)\n",
        "\n",
        "    try:\n",
        "        model.fit(x_train, y_train, \n",
        "                    epochs = E, \n",
        "                    batch_size=B,\n",
        "    #                     validation_data=(x_test, y_test),\n",
        "                    verbose=False)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"tata bye bye\")\n",
        "\n",
        "    # return w to server\n",
        "    # in our case no need for any explicit return; as the model is pass by value\n",
        "    return data.shape[0] # this will return the number of total sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxL5tN-bo2jh"
      },
      "source": [
        "# Server executes: \n",
        "def Server(loss_p):\n",
        "    # initialize w_0\n",
        "    model = Sequential()\n",
        "    \n",
        "    # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(1, kernel_initializer= 'he_uniform', activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "    # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "# for each round t = 1, 2, . . . do\n",
        "    for t in range(1, 5):\n",
        "        C = np.random.random(1)[0] # random number between 0 and 1.\n",
        "\n",
        "        # m ← max(C  K, 1)\n",
        "        m = max(int(C*K), 1) # m == random number of client selected\n",
        "        weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
        "        n_k = [None] * m # parameters for weighted sum \n",
        "\n",
        "\n",
        "        # S_t ← (random set of m clients)\n",
        "        # S = {} # dictionary\n",
        "        S_t = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "\n",
        "        # No need for below loop, as server don't need access to client data, server only need clients' number\n",
        "        # for client in m_clients:\n",
        "        #   S[client] = clients[client]\n",
        "\n",
        "        initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "        # for t= 1 to T, initial_weights would be t-1th's final weights\n",
        "\n",
        "        # for each client k ∈ S_t in parallel do\n",
        "        client_idx = 0\n",
        "        for client in S_t:\n",
        "            # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
        "            n_k[client_idx] = Client(client, model, loss_p) # pass by reference\n",
        "            weight_t_plus_1[client_idx] = model.get_weights()\n",
        "            client_idx += 1\n",
        "\n",
        "            # setting weights back to initial weights\n",
        "            model.set_weights(initial_weights)\n",
        "\n",
        "        # finding the weighted sum\n",
        "        final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0] / sum(n_k))\n",
        "\n",
        "        for idx in range(1, m):\n",
        "          # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "          final_weights_t += np.array(weight_t_plus_1[idx]) * (n_k[idx]/ sum(n_k))\n",
        "\n",
        "        # setting the aggregated weights\n",
        "        model.set_weights(final_weights_t)\n",
        "\n",
        "\n",
        "#         print(model.get_weights())\n",
        "    # return error_clients(model, loss_p)) # THIS LINE WON'T BE THERE IN REAL SERVER\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzzufX8enF4K"
      },
      "source": [
        "def error_clients(model, loss_p):\n",
        "    \n",
        "    X, y = dataset.drop('CO', axis=1),  dataset['CO']\n",
        "    \n",
        "#     for loss_p in loss_prob:\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = loss_p, random_state=0)\n",
        "   \n",
        "    # adjusting the dimensions\n",
        "    x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1,  x_test.shape[1])\n",
        "    y_test = y_test.reshape(y_test.shape[0],1,1)\n",
        "\n",
        "    y_predicted = model.predict(x_test)\n",
        "        \n",
        "    NE = sum(abs(y_test - y_predicted)) / sum(y_test)\n",
        "    return NE  \n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAyM0nH2qig3"
      },
      "source": [
        "# Final plotting\n",
        "K = 25 # as we have 25 clients\n",
        "\n",
        "loss_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "models = []\n",
        "errors = []\n",
        "\n",
        "for loss_p in loss_prob:\n",
        "  model = Server(loss_p)\n",
        "  errors.append(error_clients(model, loss_p))\n",
        "\n",
        "# import seaborn as sns\n",
        "# sns.lineplot(x = loss_prob, y = errors, markers=True, dashes=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2dbI7pP8aN0",
        "outputId": "68da4310-9bcb-4c23-f5d2-cdae1a6dc5e0"
      },
      "source": [
        "errors[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.02276379]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OimeRntH-Yax",
        "outputId": "22164916-8999-4ff8-b6cd-8bb6b6c7a698"
      },
      "source": [
        "errors[0].ravel()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0227637921798483"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhh83xyU-IEM"
      },
      "source": [
        "# reshaping errors\n",
        "for err_idx in range(len(errors)):\n",
        "  errors[err_idx] = errors[err_idx].ravel()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI6ucUOp-qKw",
        "outputId": "fc1c0b65-a69b-4746-a9f3-7fb499c0174a"
      },
      "source": [
        "errors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0227637921798483,\n",
              " 1.0231448768600002,\n",
              " 1.0219129321046316,\n",
              " 1.0227209476040346,\n",
              " 1.0226674799061215,\n",
              " 1.0220998885081014,\n",
              " 1.02239743534543,\n",
              " 1.0222275615431877,\n",
              " 1.0221784548709607]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "XOItTjqd8wKq",
        "outputId": "92d7af01-2ab1-4f21-fb4f-cb8a685a5cff"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x = loss_prob, y = errors, markers=True, dashes=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2c7e2382d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRLHH_OC-19B"
      },
      "source": [
        "def traditional_ML(loss_p):\n",
        "  # initialize w_0\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, kernel_initializer= 'he_uniform', activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "  # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "  B = 1000 #int(len(clients[client_idx]) * 0.01) # so for each client the batch size would be different depending upon the total sample size\n",
        "  E = 25 #max(30, B//10) # number of local epochs; it will also depend on sample size (indirectly)\n",
        "  learning_rate = 0.1 # initial learning rate\n",
        "  decay_rate = 0.1\n",
        "  momentum = 0.8\n",
        "\n",
        "  # define the optimizer function\n",
        "  sgd = SGD(learning_rate = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(loss='mse', \n",
        "              optimizer=sgd, \n",
        "              metrics=['mse'])\n",
        "\n",
        "\n",
        "  # for each local epoch i from 1 to E do\n",
        "  #   for batch b ∈ B do\n",
        "  #     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "  # fitting the model: same as above two for loops\n",
        "\n",
        "  # data = dataset.loc[dataset['Station code'] == client_idx]\n",
        "\n",
        "  X, y = dataset[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  dataset['CO']\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = loss_p, random_state=1)\n",
        "\n",
        "  model.fit(x_train, y_train, \n",
        "                  epochs = E, \n",
        "                  batch_size=B,\n",
        "  #                     validation_data=(x_test, y_test),\n",
        "                  verbose=False)\n",
        "      \n",
        "  x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "\n",
        "  # adjusting the dimensions\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1,  x_test.shape[1])\n",
        "  y_test = y_test.reshape(y_test.shape[0],1,1)\n",
        "  y_predicted = model.predict(x_test)\n",
        "      \n",
        "  NE = sum(abs(y_test - y_predicted)) / sum(y_test)\n",
        "  return NE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "kdKt6Vs4_tK_",
        "outputId": "0a68461c-d0bc-4513-f84a-ad0d21573e80"
      },
      "source": [
        "# Final plotting\n",
        "K = 25 # as we have 25 clients\n",
        "\n",
        "loss_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "errors = []\n",
        "\n",
        "for loss_p in loss_prob:\n",
        "  error = traditional_ML(loss_p)\n",
        "  errors.append(error)\n",
        "\n",
        "# reshaping errors\n",
        "for err_idx in range(len(errors)):\n",
        "  errors[err_idx] = errors[err_idx].ravel()[0]\n",
        "\n",
        "import seaborn as sns\n",
        "sns.lineplot(x = loss_prob, y = errors, markers=True, dashes=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_27_input'), name='dense_27_input', description=\"created by layer 'dense_27_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_30_input'), name='dense_30_input', description=\"created by layer 'dense_30_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_33_input'), name='dense_33_input', description=\"created by layer 'dense_33_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_36_input'), name='dense_36_input', description=\"created by layer 'dense_36_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_39_input'), name='dense_39_input', description=\"created by layer 'dense_39_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_42_input'), name='dense_42_input', description=\"created by layer 'dense_42_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_45_input'), name='dense_45_input', description=\"created by layer 'dense_45_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_48_input'), name='dense_48_input', description=\"created by layer 'dense_48_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 8) for input KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name='dense_51_input'), name='dense_51_input', description=\"created by layer 'dense_51_input'\"), but it was called on an input with incompatible shape (None, 1, 8).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2cb0168750>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt4LmYyAuO7q"
      },
      "source": [
        "#### for each loss prob, as it explodes memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le3Lu4Xp3NjO"
      },
      "source": [
        "# X, y = dataset.drop('CO', axis=1),  dataset['CO']\n",
        "\n",
        "# #     for loss_p in loss_prob:\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=0)\n",
        "# x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "# x_test = x_test.reshape(x_test.shape[0], 1,  x_test.shape[1])\n",
        "# # x_test[0].shape\n",
        "# y_predicted = models[0].predict(x_test)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3km4zUfU6w51"
      },
      "source": [
        "# y_predicted.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfZqB-4g60k3"
      },
      "source": [
        "# y_test = y_test.reshape(y_test.shape[0], 1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfuFcN1R6IL6"
      },
      "source": [
        "# error = sum(abs(y_test - y_predicted)) / sum(y_test)\n",
        "# # error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xPjFuF45bXw"
      },
      "source": [
        "# sum(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KznvhSE33nC"
      },
      "source": [
        "# x_test.reshape(x_test.shape[0], x_test.shape[1], 1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqlIoy67t-zW"
      },
      "source": [
        "# errors.append(error_clients(models[0], loss_prob[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw5j1OdVAbpB"
      },
      "source": [
        "# 3rd October\n",
        "* Changed the NE's denominator to absolute values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVXqRyhVAiUc",
        "outputId": "0459f1f4-78c7-44e8-d29a-8ef2fbe7668a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye-S8SqFAiUc",
        "outputId": "9fd7107f-6f73-4aa1-9608-b00c4be96513"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "# import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD # from keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import RMSprop # instead of from keras.optimizers import RMSprop\n",
        "# from keras import datasets\n",
        "\n",
        "# from keras.callbacks import LearningRateScheduler\n",
        "# from keras.callbacks import History\n",
        "\n",
        "from tensorflow.keras import losses\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n",
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvVkBCG4AiUd"
      },
      "source": [
        "# ClientUpdate(k, w): // Run on client k\n",
        "def Client(client_idx, model, loss_p):\n",
        "    # B ← (split P_k into batches of size B) # what is p_K --> each clients' sample size\n",
        "    B = 1000 #int(len(clients[client_idx]) * 0.01) # so for each client the batch size would be different depending upon the total sample size\n",
        "    E = 12 #max(30, B//10) # number of local epochs; it will also depend on sample size (indirectly)\n",
        "    learning_rate = 0.1 # initial learning rate\n",
        "    decay_rate = 0.1\n",
        "    momentum = 0.8\n",
        "\n",
        "    # define the optimizer function\n",
        "    sgd = SGD(learning_rate = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(loss='mse', \n",
        "                optimizer=sgd, \n",
        "                metrics=['mse'])\n",
        "\n",
        "\n",
        "    # for each local epoch i from 1 to E do\n",
        "    #   for batch b ∈ B do\n",
        "    #     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "    # fitting the model: same as above two for loops\n",
        "    data = clients[client_idx]\n",
        "    # data = dataset.loc[dataset['Station code'] == client_idx]\n",
        "\n",
        "    X, y = data[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  data['CO']\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = loss_p, random_state=1)\n",
        "\n",
        "    try:\n",
        "        model.fit(x_train, y_train, \n",
        "                    epochs = E, \n",
        "                    batch_size=B,\n",
        "    #                     validation_data=(x_test, y_test),\n",
        "                    verbose=False)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(\"tata bye bye\")\n",
        "\n",
        "    # return w to server\n",
        "    # in our case no need for any explicit return; as the model is pass by value\n",
        "    return data.shape[0] # this will return the number of total sample\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1aCz_H2AiUd"
      },
      "source": [
        "# Server executes: \n",
        "def Server(loss_p):\n",
        "    # initialize w_0\n",
        "    model = Sequential()\n",
        "    \n",
        "    # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(1, kernel_initializer= 'he_uniform', activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "    # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "# for each round t = 1, 2, . . . do\n",
        "    for t in range(1, 20):\n",
        "        C = np.random.random(1)[0] # random number between 0 and 1.\n",
        "\n",
        "        # m ← max(C  K, 1)\n",
        "        m = max(int(C*K), 1) # m == random number of client selected\n",
        "        weight_t_plus_1 = [None] * m # matrix to store the weights of all client in each t-round\n",
        "        n_k = [None] * m # parameters for weighted sum \n",
        "\n",
        "\n",
        "        # S_t ← (random set of m clients)\n",
        "        # S = {} # dictionary\n",
        "        S_t = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "\n",
        "        # No need for below loop, as server don't need access to client data, server only need clients' number\n",
        "        # for client in m_clients:\n",
        "        #   S[client] = clients[client]\n",
        "\n",
        "        initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "        # for t= 1 to T, initial_weights would be t-1th's final weights\n",
        "\n",
        "        # for each client k ∈ S_t in parallel do\n",
        "        client_idx = 0\n",
        "        for client in S_t:\n",
        "            # w^(k)_(t+1) ← ClientUpdate(k, model)\n",
        "            n_k[client_idx] = Client(client, model, loss_p) # pass by reference\n",
        "            weight_t_plus_1[client_idx] = model.get_weights()\n",
        "            client_idx += 1\n",
        "\n",
        "            # setting weights back to initial weights\n",
        "            model.set_weights(initial_weights)\n",
        "\n",
        "        # finding the weighted sum\n",
        "        final_weights_t = np.array(weight_t_plus_1[0]) * (n_k[0] / sum(n_k))\n",
        "\n",
        "        for idx in range(1, m):\n",
        "          # w_(t+1) ← summation(k=1 to K){(n_k/n) * w^(k)_(t+1)} #n_k - no. of training sample in each client K; n - total training samples;\n",
        "          final_weights_t += np.array(weight_t_plus_1[idx]) * (n_k[idx]/ sum(n_k))\n",
        "\n",
        "        # setting the aggregated weights\n",
        "        model.set_weights(final_weights_t)\n",
        "\n",
        "\n",
        "#         print(model.get_weights())\n",
        "    # return error_clients(model, loss_p)) # THIS LINE WON'T BE THERE IN REAL SERVER\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCX5xjzUAiUe"
      },
      "source": [
        "def error_clients(model, loss_p):\n",
        "    \n",
        "    X, y = dataset.drop('CO', axis=1),  dataset['CO']\n",
        "    \n",
        "#     for loss_p in loss_prob:\n",
        "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = loss_p, random_state=0)\n",
        "   \n",
        "    # adjusting the dimensions\n",
        "    x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1,  x_test.shape[1])\n",
        "    y_test = y_test.reshape(y_test.shape[0],1,1)\n",
        "\n",
        "    y_predicted = model.predict(x_test)\n",
        "        \n",
        "    NE = np.sum(abs(np.array(y_test) - np.array(y_predicted))) / np.sum(abs(np.array(y_test)))\n",
        "    return NE  \n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhzGibMAAiUe"
      },
      "source": [
        "%%time\n",
        "# Final plotting\n",
        "K = 25 # as we have 25 clients\n",
        "\n",
        "loss_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "models = []\n",
        "errors = []\n",
        "\n",
        "clients = {} # dictionary\n",
        "\n",
        "# reading all the clients and building model\n",
        "for lp in loss_prob:\n",
        "    for st_code in range(101,126):\n",
        "        clients[st_code] = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataNew/station_code\"+str(st_code)+\".csv\", index_col = 0)\n",
        "\n",
        "        clients[st_code]['Measurement date'] = pd.to_datetime(clients[st_code]['Measurement date'])\n",
        "\n",
        "        clients[st_code]['year'] = clients[st_code]['Measurement date'].dt.year\n",
        "        clients[st_code]['month'] = clients[st_code]['Measurement date'].dt.month\n",
        "        clients[st_code]['week'] = clients[st_code]['Measurement date'].dt.week\n",
        "        clients[st_code]['day'] = clients[st_code]['Measurement date'].dt.day\n",
        "        clients[st_code]['hour'] = clients[st_code]['Measurement date'].dt.hour\n",
        "        clients[st_code]['minute'] = clients[st_code]['Measurement date'].dt.minute # minute is not significant; as only 0 values\n",
        "        clients[st_code]['dayOfWeek'] = clients[st_code]['Measurement date'].dt.dayofweek\n",
        "\n",
        "        # choosing features\n",
        "        clients[st_code] = clients[st_code][['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek', 'CO']]\n",
        "\n",
        "\n",
        "        # splitting into train test split\n",
        "        X, y = dataset.drop('CO', axis=1),  dataset['CO']\n",
        "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = lp, random_state=0)\n",
        "        x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "        \n",
        "\n",
        "# dataset['date'], dataset['time'] = dataset['Measurement date'].dt.date, dataset['Measurement date'].dt.time\n",
        "# merging all the clients data\n",
        "import pandas as pd\n",
        "\n",
        "frames = list(clients.values())\n",
        "\n",
        "dataset = pd.concat(frames)\n",
        "display(dataset)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for loss_p in loss_prob:\n",
        "  model = Server(loss_p)\n",
        "  errors.append(error_clients(model, loss_p))\n",
        "\n",
        "# import seaborn as sns\n",
        "# sns.lineplot(x = loss_prob, y = errors, markers=True, dashes=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbxA3UOvAiUe"
      },
      "source": [
        "errors[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDTCp-_cAiUe"
      },
      "source": [
        "errors[0].ravel()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFKDWUeCAiUe"
      },
      "source": [
        "# reshaping errors\n",
        "for err_idx in range(len(errors)):\n",
        "  errors[err_idx] = errors[err_idx].ravel()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIaUbqxlAiUe"
      },
      "source": [
        "errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKiN8Je0AiUe"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(x = loss_prob, y = errors, markers=True, dashes=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvpDlGvhAiUf"
      },
      "source": [
        "def traditional_ML(loss_p):\n",
        "  # initialize w_0\n",
        "  model = Sequential()\n",
        "\n",
        "  # model.add(Dense(64, kernel_initializer='zeros', bias_initializer='zeros', activation=tf.nn.relu, input_dim=6)) # first dense/hidden layer\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu, input_dim=8))\n",
        "  model.add(Dropout(0.1))\n",
        "\n",
        "  model.add(Dense(64, kernel_initializer='he_uniform', bias_initializer=\"zeros\", activation=tf.nn.relu)) # second dense/hidden layer\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, kernel_initializer= 'he_uniform', activation=tf.nn.softmax)) # output layer\n",
        "\n",
        "  # initial_weights = model.get_weights() # setting initial weights; should be same for all clients\n",
        "\n",
        "\n",
        "  B = 1000 #int(len(clients[client_idx]) * 0.01) # so for each client the batch size would be different depending upon the total sample size\n",
        "  E = 25 #max(30, B//10) # number of local epochs; it will also depend on sample size (indirectly)\n",
        "  learning_rate = 0.1 # initial learning rate\n",
        "  decay_rate = 0.1\n",
        "  momentum = 0.8\n",
        "\n",
        "  # define the optimizer function\n",
        "  sgd = SGD(learning_rate = learning_rate, momentum = momentum, decay = decay_rate, nesterov = False)\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(loss='mse', \n",
        "              optimizer=sgd, \n",
        "              metrics=['mse'])\n",
        "\n",
        "\n",
        "  # for each local epoch i from 1 to E do\n",
        "  #   for batch b ∈ B do\n",
        "  #     w ← w − η O l(w; b) # this seems the SGD process\n",
        "\n",
        "  # fitting the model: same as above two for loops\n",
        "\n",
        "  # data = dataset.loc[dataset['Station code'] == client_idx]\n",
        "\n",
        "  X, y = dataset[['Latitude', 'Longitude','year', 'month', 'week', 'day', 'hour', 'dayOfWeek']],  dataset['CO']\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = loss_p, random_state=1)\n",
        "\n",
        "  model.fit(x_train, y_train, \n",
        "                  epochs = E, \n",
        "                  batch_size=B,\n",
        "  #                     validation_data=(x_test, y_test),\n",
        "                  verbose=False)\n",
        "      \n",
        "  x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "\n",
        "  # adjusting the dimensions\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1,  x_test.shape[1])\n",
        "  y_test = y_test.reshape(y_test.shape[0],1,1)\n",
        "  y_predicted = model.predict(x_test)\n",
        "      \n",
        "  NE = np.sum(abs(np.array(y_test) - np.array(y_predicted))) / np.sum(abs(np.array(y_test)))\n",
        "  return NE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAsrCj35AiUf"
      },
      "source": [
        "# Final plotting\n",
        "K = 25 # as we have 25 clients\n",
        "\n",
        "loss_prob = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "errors = []\n",
        "\n",
        "for loss_p in loss_prob:\n",
        "  error = traditional_ML(loss_p)\n",
        "  errors.append(error)\n",
        "\n",
        "# reshaping errors\n",
        "for err_idx in range(len(errors)):\n",
        "  errors[err_idx] = errors[err_idx].ravel()[0]\n",
        "\n",
        "import seaborn as sns\n",
        "sns.lineplot(x = loss_prob, y = errors, markers=True, dashes=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_340Qw5UAiUf"
      },
      "source": [
        "#### for each loss prob, as it explodes memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH1TX4RXAiUf"
      },
      "source": [
        "# X, y = dataset.drop('CO', axis=1),  dataset['CO']\n",
        "\n",
        "# #     for loss_p in loss_prob:\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=0)\n",
        "# x_test, y_test = x_test.to_numpy(), y_test.to_numpy()\n",
        "# x_test = x_test.reshape(x_test.shape[0], 1,  x_test.shape[1])\n",
        "# # x_test[0].shape\n",
        "# y_predicted = models[0].predict(x_test)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C3Ojxg_AiUf"
      },
      "source": [
        "# y_predicted.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgfSf2IbAiUf"
      },
      "source": [
        "# y_test = y_test.reshape(y_test.shape[0], 1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqxKbwzNAiUf"
      },
      "source": [
        "# error = sum(abs(y_test - y_predicted)) / sum(y_test)\n",
        "# # error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJblAobBAiUf"
      },
      "source": [
        "# sum(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAfPQ_98AiUg"
      },
      "source": [
        "# x_test.reshape(x_test.shape[0], x_test.shape[1], 1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSVRUrd2AiUg"
      },
      "source": [
        "# errors.append(error_clients(models[0], loss_prob[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EizJ6Dty6ux7"
      },
      "source": [
        "# Trial for FL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHK9VB1f-olk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e461cd7-83a6-4473-843b-5feae35159f6"
      },
      "source": [
        "# ## generating random number between start to end\n",
        "# import numpy as np\n",
        " \n",
        "# start = 101   # inclusive\n",
        "# end = 125    # exclusive\n",
        "\n",
        "# n = 12\n",
        "\n",
        "# x = np.random.uniform(low=101, high=126, size=(m)).astype(int)\n",
        "# print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[118 108 113 118 101 105 116 124 119 105 103 111]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nQD9MQx6-ay"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkeRI8uH_XYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247356cf-316b-47e2-ae80-6a36e5134c72"
      },
      "source": [
        "## clients\n",
        "clients = []\n",
        "\n",
        "for i in range(10): # number of clients = 10\n",
        "  client = []\n",
        "  client.append(np.random.uniform(low=1, high=12, size=(5)).astype(int))\n",
        "  client.append(2*client[0] + 5)\n",
        "  clients.append(client)\n",
        "\n",
        "weight_t_plus_1 = [None] * 10 #10 represents the number of clients\n",
        "\n",
        "clients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[array([10,  5,  9,  1,  6]), array([25, 15, 23,  7, 17])],\n",
              " [array([ 4,  3,  3, 11,  6]), array([13, 11, 11, 27, 17])],\n",
              " [array([ 3,  3, 10,  9,  7]), array([11, 11, 25, 23, 19])],\n",
              " [array([ 6,  8,  9,  3, 11]), array([17, 21, 23, 11, 27])],\n",
              " [array([11,  4,  3,  7, 11]), array([27, 13, 11, 19, 27])],\n",
              " [array([ 8, 11,  9,  8,  1]), array([21, 27, 23, 21,  7])],\n",
              " [array([5, 9, 8, 1, 7]), array([15, 23, 21,  7, 19])],\n",
              " [array([ 2,  5, 11,  4,  8]), array([ 9, 15, 27, 13, 21])],\n",
              " [array([3, 7, 8, 1, 6]), array([11, 19, 21,  7, 17])],\n",
              " [array([2, 6, 6, 3, 2]), array([ 9, 17, 17, 11,  9])]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgC00t626zmA"
      },
      "source": [
        "def client(X, y, model):\n",
        "  adam = Adam(learning_rate = 0.01)\n",
        "  model.compile(optimizer = adam,\n",
        "              loss = 'mse',\n",
        "              metrics = ['mse'])\n",
        "  print(\"Before Training\")\n",
        "  print(model.get_weights())\n",
        "  print()\n",
        "  model.fit(X, y, epochs = 10, verbose=False)\n",
        "  print(\"After Training\")\n",
        "  print(model.get_weights())\n",
        "  print()\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12GC6RQd7Msi"
      },
      "source": [
        "def server():\n",
        "  # Defining the architecture of NN model\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Dense(5, input_dim = 1, activation = 'linear'))\n",
        "  # model.add(Dense(5,  activation = 'linear'))\n",
        "  print(\"initial weight :\", model.get_weights())\n",
        "  print()\n",
        "\n",
        "  # defining weight matrix for t+1th roung\n",
        "  # weight_t_plus_1 = [None] * 10 #10 represents the number of clients\n",
        "\n",
        "\n",
        "\n",
        "  import numpy as np\n",
        "\n",
        "  # update\n",
        "  initial_weights = model.get_weights() # setting initial weights\n",
        "\n",
        "  for i in range(10):\n",
        "    client(clients[i][0], clients[i][1], model) # pass by reference\n",
        "    print(str(i),\"th model's weights\", model.get_weights())\n",
        "    \n",
        "    weight_t_plus_1[i] = model.get_weights()\n",
        "\n",
        "    print()\n",
        "\n",
        "    # setting weights back to initial weights\n",
        "    model.set_weights(initial_weights)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbQdwcBz7ULp"
      },
      "source": [
        "## Server running\n",
        "server()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddvyRRMb7fpX"
      },
      "source": [
        "## adding/ all the weights\n",
        "weights = np.array(weight_t_plus_1[0])\n",
        "\n",
        "for i in range(1,10):\n",
        "  weights += np.array(weight_t_plus_1[i])\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXTQltLx8QJz",
        "outputId": "a57a8501-d700-409e-f518-3efb5887d918"
      },
      "source": [
        "# checking whether we can add the elements\n",
        "a = 0\n",
        "for i in range(10):\n",
        "  a += weight_t_plus_1[i][0][0][1]\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-6.675058722496033"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGDSctyf8jwG"
      },
      "source": [
        "# checking if 0.5 gets multiplied will all the numbers\n",
        "np.array(weight_t_plus_1[1])* 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITXwndrTnrK8"
      },
      "source": [
        "# merging all the data frames\n",
        "import pandas as pd\n",
        "# First DataFrame\n",
        "df1 = pd.DataFrame({'id': ['A01', 'A02', 'A03', 'A04'],\n",
        "\t\t\t\t\t'Name': ['ABC', 'PQR', 'DEF', 'GHI']})\n",
        "\n",
        "# Second DataFrame\n",
        "df2 = pd.DataFrame({'id': ['B05', 'B06', 'B07', 'B08'],\n",
        "\t\t\t\t\t'Name': ['XYZ', 'TUV', 'MNO', 'JKL']})\n",
        "\n",
        "df3 = pd.DataFrame({'id': ['B05', 'B06', 'B07', 'B08'],\n",
        "\t\t\t\t\t'Name': ['XYZ', 'TUV', 'MNO', 'JKL']})\n",
        "\n",
        "frames = [df1, df2, df3]\n",
        "\n",
        "result = pd.concat(frames)\n",
        "display(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}